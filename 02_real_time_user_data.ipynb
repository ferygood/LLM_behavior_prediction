{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7kiKAtMWv6FAzBsXK9RV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferygood/LLM_behavior_prediction/blob/main/02_real_time_user_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhtSp5Ec43OT"
      },
      "outputs": [],
      "source": [
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you have to install and run Kafka ([Quick Start](https://kafka.apache.org/quickstart) ).\n",
        "\n",
        "\n",
        "1.   Kafka has reset mechanism which prevent data lost when sending and receving.\n",
        "2.   Kafka has partitions to handle large data size\n",
        "3.   kafka will keep the data from some time to prevent data lost.\n",
        "\n"
      ],
      "metadata": {
        "id": "jBSJawXEivjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaProducer\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# configure Kafka Producer\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=['localhost:9092'],\n",
        "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        ")\n",
        "\n",
        "# simulate sending data to Kafka Producer\n",
        "def generate_and_send_data():\n",
        "    while True:\n",
        "        web_visit = {\n",
        "            'user_id': int(np.random.randint(1, 1000)),\n",
        "            'visit_time': pd.Timestamp.now().isoformat(),\n",
        "            'page_url': np.random.choice(['home', 'product', 'cart', 'checkout']),\n",
        "            'referrer_url': np.random.choice(['google', 'facebook', 'twitter', 'direct'])\n",
        "        }\n",
        "\n",
        "        purchase = {\n",
        "            'user_id': int(np.random.randint(1, 1000)),\n",
        "            'purchase_time': pd.Timestamp.now().isoformat(),\n",
        "            'product_id': int(np.random.randint(1, 100)),\n",
        "            'amount': float(np.random.uniform(10, 500))\n",
        "        }\n",
        "\n",
        "        social_interaction = {\n",
        "            'user_id': int(np.random.randint(1, 1000)),\n",
        "            'interaction_time': pd.Timestamp.now().isoformat(),\n",
        "            'platform': np.random.choice(['facebook', 'twitter', 'instagram']),\n",
        "            'action': np.random.choice(['like', 'share', 'comment'])\n",
        "        }\n",
        "\n",
        "        producer.send('web_visit_topic', web_visit)\n",
        "        producer.send('purchase_topic', purchase)\n",
        "        producer.send('social_interaction_topic', social_interaction)\n",
        "\n",
        "        time.sleep(1)  # give a timeframe for generating data\n",
        "\n",
        "# generate and send data\n",
        "generate_and_send_data()\n"
      ],
      "metadata": {
        "id": "LXMdgBMR6MFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Received data from Kafka and pre-process the data"
      ],
      "metadata": {
        "id": "_aezLiPijW6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaConsumer\n",
        "\n",
        "# set Kafka Consumer\n",
        "consumer_web_visit = KafkaConsumer(\n",
        "    'web_visit_topic',\n",
        "    bootstrap_servers=['localhost:9092'],\n",
        "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
        ")\n",
        "\n",
        "consumer_purchase = KafkaConsumer(\n",
        "    'purchase_topic',\n",
        "    bootstrap_servers=['localhost:9092'],\n",
        "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
        ")\n",
        "\n",
        "consumer_social_interaction = KafkaConsumer(\n",
        "    'social_interaction_topic',\n",
        "    bootstrap_servers=['localhost:9092'],\n",
        "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
        ")\n",
        "\n",
        "# pre-process the data\n",
        "def process_data():\n",
        "    web_visit_data = []\n",
        "    purchase_data = []\n",
        "    social_interaction_data = []\n",
        "\n",
        "    for message in consumer_web_visit:\n",
        "        web_visit_data.append(message.value)\n",
        "\n",
        "        # create tome feature column\n",
        "        if len(web_visit_data) >= 10:\n",
        "            df = pd.DataFrame(web_visit_data)\n",
        "            df.drop_duplicates(inplace=True)\n",
        "            df['visit_time'] = pd.to_datetime(df['visit_time'])\n",
        "            df['visit_date'] = df['visit_time'].dt.date\n",
        "            df['visit_hour'] = df['visit_time'].dt.hour\n",
        "            print(\"Processed web visit data:\\n\", df.head())\n",
        "            web_visit_data = []\n",
        "\n",
        "    for message in consumer_purchase:\n",
        "        purchase_data.append(message.value)\n",
        "\n",
        "        if len(purchase_data) >= 10:\n",
        "            df = pd.DataFrame(purchase_data)\n",
        "            df.drop_duplicates(inplace=True)\n",
        "            df['purchase_time'] = pd.to_datetime(df['purchase_time'])\n",
        "            df['purchase_date'] = df['purchase_time'].dt.date\n",
        "            df['purchase_hour'] = df['purchase_time'].dt.hour\n",
        "            print(\"Processed purchase data:\\n\", df.head())\n",
        "            purchase_data = []\n",
        "\n",
        "    for message in consumer_social_interaction:\n",
        "        social_interaction_data.append(message.value)\n",
        "\n",
        "        if len(social_interaction_data) >= 10:\n",
        "            df = pd.DataFrame(social_interaction_data)\n",
        "            df.drop_duplicates(inplace=True)\n",
        "            df['interaction_time'] = pd.to_datetime(df['interaction_time'])\n",
        "            df['interaction_date'] = df['interaction_time'].dt.date\n",
        "            df['interaction_hour'] = df['interaction_time'].dt.hour\n",
        "            print(\"Processed social interaction data:\\n\", df.head())\n",
        "            social_interaction_data = []\n",
        "\n",
        "# pre-process data\n",
        "process_data()\n"
      ],
      "metadata": {
        "id": "aYXBvr_ijdHr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}